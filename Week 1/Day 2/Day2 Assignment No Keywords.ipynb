{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The 'Michael Eisner Memorial Weak Executive Problem' refers to the phenomenon where a CEO or startup founder hires someone weak into an executive role for a function they previously excelled in, possibly due to a reluctance to let go of that function. This can result in hiring executives who are not as strong or competent as needed for the role, ultimately impacting the company's success.\n",
      "\n",
      "Context:\n",
      "Source: Text File (chunk 21), Relevance: 1.00\n",
      "Text preview: output– accomplishment. Validate it by reference checking peers, reports, and bosses. Along the way, reference check personality and teamwork, but look Xrst and foremost for a pattern of output. Fiah,...\n",
      "==================================================\n",
      "Source: PDF File (chunk 21), Relevance: 1.00\n",
      "Text preview: succceed without him, then your ideal executive hire is someone who will succeed without you. • Beware hiring a big company executive for a startup.The executive skill sets required for a big company ...\n",
      "==================================================\n",
      "Source: Text File (chunk 24), Relevance: 0.50\n",
      "Text preview: has a much deeper problem and you just dodged a huge bullet. And on that cheery note, good luck! Counterpoint: Ben Horowitz on micromanagement [This is a guest post from my business partner Ben Horowi...\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import fitz\n",
    "import re\n",
    "\n",
    "def chunk_text(text, max_tokens=1000, overlap=200):\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i + max_tokens]) for i in range(0, len(words), max_tokens - overlap)]\n",
    "\n",
    "class DocumentLoader:\n",
    "    def load(self, path):\n",
    "        return open(path, 'r', encoding='utf-8').read() if path.endswith('.txt') else '\\n'.join(page.get_text() for page in fitz.open(path))\n",
    "\n",
    "class VectorDB:\n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "        self.data = []\n",
    "        self.stop_words = set(['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"])\n",
    "\n",
    "    def add(self, docs, metadata):\n",
    "        for doc, meta in zip(docs, metadata):\n",
    "            for i, chunk in enumerate(chunk_text(doc)):\n",
    "                embedding = self.embed(chunk)\n",
    "                if embedding:\n",
    "                    self.data.append({\"embedding\": embedding, \"metadata\": f\"{meta} (chunk {i+1})\", \"text\": chunk})\n",
    "\n",
    "    def embed(self, text):\n",
    "        try:\n",
    "            return self.client.embeddings.create(input=text, model=\"text-embedding-ada-002\").data[0].embedding\n",
    "        except Exception as e:\n",
    "            print(f\"Embedding error: {e}\")\n",
    "            return []\n",
    "\n",
    "    def extract_keywords(self, text):\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        return [word for word in words if word not in self.stop_words]\n",
    "\n",
    "    def search(self, query, k=3):\n",
    "        keywords = self.extract_keywords(query)\n",
    "        relevant_chunks = []\n",
    "        \n",
    "        for d in self.data:\n",
    "            score = sum(keyword in d[\"text\"].lower() for keyword in keywords) / len(keywords)\n",
    "            if score > 0:\n",
    "                relevant_chunks.append((d[\"metadata\"], d[\"text\"], score))\n",
    "        \n",
    "        if len(relevant_chunks) < k:\n",
    "            q_embed = self.embed(query)\n",
    "            if q_embed:\n",
    "                sims = cosine_similarity([q_embed], [d[\"embedding\"] for d in self.data])[0]\n",
    "                sorted_indices = sims.argsort()[::-1]\n",
    "                for i in sorted_indices:\n",
    "                    if len(relevant_chunks) >= k:\n",
    "                        break\n",
    "                    if (self.data[i][\"metadata\"], self.data[i][\"text\"], sims[i]) not in relevant_chunks:\n",
    "                        relevant_chunks.append((self.data[i][\"metadata\"], self.data[i][\"text\"], sims[i]))\n",
    "        \n",
    "        return sorted(relevant_chunks, key=lambda x: x[2], reverse=True)[:k]\n",
    "\n",
    "class SystemRolePrompt:\n",
    "    def __init__(self, template):\n",
    "        self.template = template\n",
    "    def create_message(self):\n",
    "        return {\"role\": \"system\", \"content\": self.template}\n",
    "\n",
    "class UserRolePrompt:\n",
    "    def __init__(self, template):\n",
    "        self.template = template\n",
    "    def create_message(self, **kwargs):\n",
    "        return {\"role\": \"user\", \"content\": self.template.format(**kwargs)}\n",
    "\n",
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "You are a helpful AI assistant tasked with answering questions based on the provided context.\n",
    "Use the following guidelines:\n",
    "1. Only use information from the given context to answer the query.\n",
    "2. If the context doesn't contain relevant information, respond with \"I don't have enough information to answer this question.\"\n",
    "3. Provide concise and accurate answers, citing the source (metadata) when possible.\n",
    "4. If the information is ambiguous or contradictory, explain the discrepancies.\n",
    "5. Do not make up or infer information that is not present in the context.\n",
    "\"\"\"\n",
    "\n",
    "USER_PROMPT_TEMPLATE = \"\"\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "User Query: {user_query}\n",
    "\n",
    "Please provide a clear and concise answer based on the above context.\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = SystemRolePrompt(RAG_PROMPT_TEMPLATE)\n",
    "user_prompt = UserRolePrompt(USER_PROMPT_TEMPLATE)\n",
    "\n",
    "class RetrievalAugmentedQAPipeline:\n",
    "    def __init__(self, llm: OpenAI, vector_db: VectorDB):\n",
    "        self.llm = llm\n",
    "        self.vector_db = vector_db\n",
    "\n",
    "    def run_pipeline(self, user_query: str) -> dict:\n",
    "        context_list = self.vector_db.search(user_query, k=3)\n",
    "        context_prompt = \"\\n\\n\".join([f\"Source: {meta} (Relevance: {sim:.2f})\\n{text}\" for meta, text, sim in context_list])\n",
    "        formatted_system_prompt = rag_prompt.create_message()\n",
    "        formatted_user_prompt = user_prompt.create_message(user_query=user_query, context=context_prompt)\n",
    "        try:\n",
    "            response = self.llm.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[formatted_system_prompt, formatted_user_prompt],\n",
    "                max_tokens=300\n",
    "            )\n",
    "            answer = response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response: {e}\")\n",
    "            answer = \"I'm sorry, I encountered an error while processing your query.\"\n",
    "        return {\"response\": answer, \"context\": [(meta, text[:200] + \"...\", sim) for meta, text, sim in context_list]}\n",
    "\n",
    "def main():\n",
    "    api_key = input(\"Enter OpenAI API key: \").strip()\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    loader = DocumentLoader()\n",
    "    db = VectorDB(client)\n",
    "    txt_doc = loader.load(\"/Users/annatucker/AEI4/Week 1/Day 2/data/PMarcaBlogs.txt\")\n",
    "    pdf_doc = loader.load(\"/Users/annatucker/AEI4/Week 1/Day 2/data/The-pmarca-Blog-Archives copy.pdf\")\n",
    "    db.add([txt_doc, pdf_doc], [\"Text File\", \"PDF File\"])\n",
    "    pipeline = RetrievalAugmentedQAPipeline(client, db)\n",
    "    query = \"What is the 'Michael Eisner Memorial Weak Executive Problem'?\"\n",
    "    result = pipeline.run_pipeline(query)\n",
    "    print(f\"Response: {result['response']}\\n\")\n",
    "    print(\"Context:\")\n",
    "    for meta, text, sim in result['context']:\n",
    "        print(f\"Source: {meta}, Relevance: {sim:.2f}\")\n",
    "        print(f\"Text preview: {text}\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmops-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
